# -*- coding: utf-8 -*-
"""Project-257_Resume Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AfC1NhbJkxawfz6W2GPD45CpNZ6wq_N0

# Project-257_Resume Classification

Business objective-


The document classification solution should significantly reduce the manual human effort in the HRM. It should achieve a higher level of accuracy and automation with minimal human intervention

# **1.Import Libraries**
"""

import warnings
warnings.filterwarnings('ignore')

pip install textract

import pandas as pd
import os
import textract

from google.colab import drive
drive.mount('/content/drive/')

"""# IMPORT DATA"""

os.listdir("/content/drive/MyDrive/P-257")

"""# **2.Extract The Text From Dataset**

**For Internship**
"""

file_path1 = []
category1 = []
directory1 = '/content/drive/MyDrive/P-257/Internship'

for i in os.listdir(directory1):
    if i.endswith('.docx'):
        file = os.path.join(directory1, i)
        extracted_text = textract.process(file).decode('utf-8')
        file_path1.append(extracted_text)
        category1.append('Internship')

data1 = pd.DataFrame(data = file_path1 , columns = ['Raw_Details'])
data1['Category1'] = category1
data1

"""**For PeopleSoft Resumes**"""

file_path2 = []
category2 = []
directory2 = '/content/drive/MyDrive/P-257/Peoplesoft resumes'

for i in os.listdir(directory2):
    if i.endswith('.docx'):
        file = os.path.join(directory2, i)
        extracted_text = textract.process(file).decode('utf-8')
        file_path2.append(extracted_text)
        category2.append('Peoplesoft resumes')

data2 = pd.DataFrame(data = file_path2 , columns = ['Raw_Details'])
data2['Category2'] = category2
data2

"""**For React JS Developer**"""

file_path3 = []
category3 = []
directory3 = '/content/drive/MyDrive/P-257/React JS developer'

for i in os.listdir(directory3):
    if i.endswith('.docx'):
        file = os.path.join(directory3, i)
        extracted_text = textract.process(file).decode('utf-8')
        file_path3.append(extracted_text)
        category3.append('React JS developer')

data3 = pd.DataFrame(data = file_path3 , columns = ['Raw_Details'])
data3['Category3'] = category3
data3

"""**For SQL Developer Lightning Insight**"""

file_path4 = []
category4 = []
directory4 = '/content/drive/MyDrive/P-257/SQL Developer Lightning insight'

for i in os.listdir(directory4):
  if i.endswith('.docx'):
    file = os.path.join(directory4,i)
    extracted_text =textract.process(file).decode('utf-8')
    file_path4.append(extracted_text)
    category4.append('SQL Developer Lightning insight')

data4 = pd.DataFrame(data = file_path4 , columns = ['Raw_Details'])
data4['Category4'] = category4
data4

"""**For Workday Resumes**"""

file_path5 = []
category5 = []
directory5 = '/content/drive/MyDrive/P-257/workday resumes'

for i in os.listdir(directory5):
  if i.endswith('.docx'):
    file = os.path.join(directory5,i)
    extracted_text =textract.process(file).decode('utf-8')
    file_path5.append(extracted_text)
    category5.append('workday resumes')

data5 = pd.DataFrame(data = file_path5 , columns = ['Raw_Details'])
data5['Category5'] = category5
data5

"""**For React Developer**"""

file_path6 = []
category6 = []
directory6 = '/content/drive/MyDrive/P-257/React Developer'

for i in os.listdir(directory6):
    if i.endswith('.docx'):
        file = os.path.join(directory6, i)
        extracted_text = textract.process(file).decode('utf-8')
        file_path6.append(extracted_text)
        category6.append('React Developer')

data6 = pd.DataFrame(data = file_path6 , columns = ['Raw_Details'])
data6['Category6'] = category6
data6

"""**3.Create a DataFrame**"""

resume_data = data1.append([data2, data3, data4, data5, data6], ignore_index = True)
resume_data

resume_data.info()

"""**Merge All The unnecessary column in one column**"""

resume_data['Category'] = category1 + category2 + category3 + category4 + category5 + category6
resume_data

resume_data.drop(['Category1' , 'Category2' , 'Category3' , 'Category4' , 'Category5' ,'Category6' ], axis = 1,inplace = True)
resume_data = resume_data[["Category" , "Raw_Details"]]

"""**Final Dataset**"""

resume_data.head()

resume_data.tail()

resume_data.shape

resume_data

resume_data["Raw_Details"]

"""**Save New CSV file**"""

resume_data.to_csv('Raw_Resume.csv' , index=False)

"""**4.Data Understanding**"""

resume_data.to_csv('Raw_resume.csv')
resume_data

resume_data[resume_data.Category == 'Internship']

resume_data[resume_data.Category == 'Peoplesoft resumes']

resume_data[resume_data.Category == 'React JS developer']

resume_data[resume_data.Category == 'SQL Developer Lightning insight']

resume_data[resume_data.Category == 'workday resumes']

resume_data[resume_data.Category == 'React Developer']

resume_data.isnull().sum()

import warnings
warnings.filterwarnings('ignore')

"""**Number of Words in Each Resume**"""

resume_data['Word_Count'] = resume_data['Raw_Details'].apply(lambda x: len(str(x).split(" ")))
resume_data[['Raw_Details','Word_Count']].head()

resume_data['Word_Count'] = resume_data['Raw_Details'].apply(lambda x: len(str(x).split(" ")))
resume_data[['Raw_Details','Word_Count']].tail()

"""**Number Of Characters**"""

resume_data['Char_Count'] = resume_data['Raw_Details'].str.len()
resume_data[['Raw_Details','Char_Count']].head()

resume_data['Char_Count'] = resume_data['Raw_Details'].str.len()
resume_data[['Raw_Details','Char_Count']].tail()

"""**Number Of stopwords**"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop = stopwords.words('english')
resume_data['Stopwords'] = resume_data['Raw_Details'].apply(lambda x: len([x for x in x.split() if x in stop]))
resume_data[['Raw_Details','Stopwords']].head()

resume_data['Stopwords'] = resume_data['Raw_Details'].apply(lambda x: len([x for x in x.split() if x in stop]))
resume_data[['Raw_Details','Stopwords']].tail()

"""**Number of Numerics**"""

resume_data['Numerics'] = resume_data['Raw_Details'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
resume_data[['Raw_Details','Numerics']].head()

resume_data['Numerics'] = resume_data['Raw_Details'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
resume_data[['Raw_Details','Numerics']].tail()

resume_data

"""# **5.Text Pre-Processing**

**Preprocesses  a sentence by performing various text cleaning and filtering operations.**


**Using Regular Expression**
"""

from nltk.classify.rte_classify import RegexpTokenizer
def preprocess(sentence):
  sentence = str(sentence) #Converts the input sentence to a string data type
  sentence = sentence.lower() #Converts all characters in the sentence to lowercase.
  sentence = sentence.replace('{html}'," ") # Removes the string '{html}' from the sentence.
  cleanr = re.compile('<.*?>') #Compiles a regular expression pattern to match and remove HTML tags.
  cleantext = re.sub(cleanr, '', sentence) #Uses the compiled pattern to remove HTML tags from the sentence.
  rem_url = re.sub(r'http\S+', '',cleantext) # Removes URLs (starting with 'http' or 'https') from the sentence.
  rem_num = re.sub('[0-9]+','',rem_url) #Removes any numeric digits from the sentence.
  tokenizer = RegexpTokenizer(r'\w+') #Creates a tokenizer using a regular expression pattern that matches word characters
  tokens = tokenizer.tokenize(rem_num) # Tokenizes the sentence into individual words using the tokenizer.
  filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')] #Filters out words with a length less than or equal to 2 and removes common English stopwords using the NLTK library.


  return " ".join(filtered_words) #Joins the filtered words back into a single string, separated by a space.

import re
from nltk.tokenize import RegexpTokenizer
resume_data = pd.read_csv('Raw_Resume.csv')
resume_data['Resume_Details'] = resume_data.Raw_Details.apply(lambda x: preprocess(x))

"""**Clean Text From Resume**"""

resume_data

"""**Saving the Clean Data in New CSV File**"""

resume_data.drop(['Raw_Details'], axis = 1, inplace = True)
resume_data

resume_data.to_csv('Cleaned_Resumes.csv', index = False)

resume_data = pd.read_csv('Cleaned_Resumes.csv')
resume_data

resume_data.Resume_Details

Details = resume_data.Resume_Details
Details

"""**6.Name Entity Recognition**"""

import string

import nltk
nltk.download('punkt')

oneSetOfStopWords = set(stopwords.words('english')+['``',"''"])
totalWords =[]
Sentences = resume_data['Resume_Details'].values
cleanedSentences = ""
for records in Sentences:
    cleanedText = preprocess(records)
    cleanedSentences += cleanedText
    requiredWords = nltk.word_tokenize(cleanedText)
    for word in requiredWords:
        if word not in oneSetOfStopWords and word not in string.punctuation:
            totalWords.append(word)

wordfreqdist = nltk.FreqDist(totalWords)
mostcommon = wordfreqdist.most_common()
print(mostcommon)

"""**Joining the list into one String**"""

text = ' '.join(Details)
text

from nltk.tokenize import word_tokenize
text_tokens = word_tokenize(text)
text_tokens

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag

def filter_nouns_and_verbs(text):
  sentences = sent_tokenize(text)
  filtered_tokens = []
  for sentence in sentences:
    tokens = word_tokenize(sentence)
    tagged_tokens = pos_tag(tokens)
    filtered_tokens.extend(word for word,pos in tagged_tokens if pos.startswith('NN') or pos.startswith('VB'))
    return filtered_tokens

nltk.download('averaged_perceptron_tagger')

nouns_verbs = filter_nouns_and_verbs(text)
print(nouns_verbs)

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X = cv.fit_transform(nouns_verbs)
sum_words = X.sum(axis=0)

words_freq = [(word,sum_words[0,idx]) for word, idx in cv.vocabulary_.items()]
words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)


wd_df = pd.DataFrame(words_freq)
wd_df.columns = ['Words','Count']
wd_df[0:15]

import matplotlib.pylab as pylab

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from wordcloud import WordCloud, STOPWORDS

fig, axe = plt.subplots(1,1, figsize=(10,7), dpi=200)
ax = sns.barplot(x= wd_df['Count'].head(20), y= wd_df.Words.head(20), data= wd_df, ax = axe,
            label= 'Total Pofile Category : {}'.format(len(resume_data.Category.unique())))

axe.set_xlabel('Frequency', size=16,fontweight= 'bold')
axe.set_ylabel('Words', size=16, fontweight= 'bold')
plt.xticks(rotation = 0)
plt.legend(loc='best', fontsize= 'x-large')
plt.title('Most Used Words count in Resumes', fontsize= 18, fontweight= 'bold')
rcParams = {'xtick.labelsize':'14','ytick.labelsize':'14','axes.labelsize':'16'}

for i in ax.containers:
    ax.bar_label(i,color = 'black', fontweight = 'bold', fontsize= 12)

pylab.rcParams.update(rcParams)
fig.tight_layout()
plt.show()

text = " ".join(x for x in wd_df.Words)
text

def plot_cloud(worldcloud):
  plt.figure(figsize=(15, 30))
  plt.imshow(wordcloud)
  plt.axis("off");

stopwords=STOPWORDS
stopwords

resume_data['Category'].value_counts().sort_index().plot(kind='bar', figsize=(15, 7) )
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('seaborn-dark-palette')
plt.figure(figsize=(15,7))
plt.title("The distinct categories of resumes")
plt.xticks(rotation=90)
sns.countplot(y="Category", data=resume_data,color=None)
plt.show()

resume_data['Category'].value_counts().sort_index().plot(kind='pie', figsize=(12, 6) )
plt.show()

pip install wordcloud

from wordcloud import WordCloud

wordcloud = WordCloud(width=3000, height=2000, background_color='black')
wordcloud.generate(text)

# Plot and display the word cloud
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bicubic')
plt.axis('off')
plt.show()

"""###########################################################################################################################

# EDA-Exploratory Data Analyisis

*   EDA involves asking questions about the data, examining its properties, and visualizing it in different ways to reveal interesting and meaningful information. EDA allows us to explore the data and make initial observations before diving into more advanced analysis or modeling.
*   The goal of EDA is to gain insights and generate hypotheses about the data, which can guide further analysis or decision-making. It helps in understanding the data's limitations, potential biases, and identifying areas for further investigation.
"""

pip install docx2txt

# Commented out IPython magic to ensure Python compatibility.
import os
import spacy
import docx2txt
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab

from textblob import TextBlob
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
from sklearn.feature_extraction.text import CountVectorizer

import warnings
warnings.filterwarnings('ignore')

"""**About Dataset**"""

file_path = '/content/drive/MyDrive/P-257'
doc_file = []
pdf_file = []
docx_file = []
folder_name = []

for root, folders, files in os.walk(file_path):
    for file in files:
        file_extension = os.path.splitext(file)[1].lower()
        folder_name.append(root)

        if file_extension == '.doc':
            doc_file.append(file)
        elif file_extension == '.docx':
            docx_file.append(file)
        elif file_extension == '.pdf':
            pdf_file.append(file)

# Print the results
print("DOC files:")
for folder, file in zip(folder_name, doc_file):
    print(f"Folder: {folder}\tFile: {file}")

print("\nDOCX files:")
for folder, file in zip(folder_name, docx_file):
    print(f"Folder: {folder}\tFile: {file}")

print("\nPDF files:")
for folder, file in zip(folder_name, pdf_file):
    print(f"Folder: {folder}\tFile: {file}")

print('Number of .doc Files  = {}'.format(len(doc_file)),'\n'
      'Number of .pdf Files  = {}'.format(len(pdf_file)),'\n'
      'Number of .docx Files = {}'.format(len(docx_file)))

print("Total Number of Files = ", len(docx_file)+len(doc_file)+len(pdf_file))

"""**Type Of File Format**"""

rcParams = {'xtick.labelsize':'14','ytick.labelsize':'14','axes.labelsize':'16'}

fig, axe = plt.subplots(1,1, figsize=(9,6), dpi=100)
ax = sns.barplot(x=['.docx File','.doc File','.pdf File'], y= [len(docx_file),len(doc_file),len(pdf_file)], width=0.5,
                 ax = axe, label= 'Total Resumes = {}'.format(len(docx_file)+len(doc_file)+len(pdf_file)))

axe.set_xlabel('Extensions', size=16,fontweight = 'bold')
axe.set_ylabel('Frequency', size=16,fontweight = 'bold')
plt.legend(loc='best', fontsize= 'large')
plt.title('Type of Files in Resumes', fontsize= 18, fontweight= 'bold')

for i in ax.containers:
    ax.bar_label(i,color = 'black', fontweight = 'bold', fontsize= 12)

pylab.rcParams.update(rcParams)
fig.tight_layout()
plt.show()
fig.savefig('IMG\File_Type_Bar', dpi = 500)

fig = plt.figure(figsize=(8,8), dpi = 100)

sizes = [len(docx_file),len(doc_file),len(pdf_file)]
labels = ['.docx Files','.doc Files','.pdf File']
colors = ['Yellow', 'cyan', 'Red']
explode = (0.00, 0.00, 0.01)

plt.pie(sizes, colors= colors, labels= labels, autopct= '%1.0f%%', pctdistance= 0.85,
        explode= explode, startangle= 0, textprops= {'size':'large', 'fontweight':'bold'})

centre_circle = plt.Circle((0,0), 0.60, fc='w')
fig.gca().add_artist(centre_circle)
plt.title('Percentage of Extensions in Resumes', fontsize= 18, fontweight= 'bold')
plt.legend(labels, loc= "center")

pylab.rcParams.update(rcParams)
fig.tight_layout()
plt.show()
fig.savefig('IMG\Per_File_Pai', dpi = 500)

"""**Extarct Resumes By Resume Category**"""

file_path = '/content/drive/MyDrive/P-257'
file_name = []
profile = []

for folder in os.listdir(file_path):
    folder_path = os.path.join(file_path, folder)  # Include full folder path
    for file in os.listdir(folder_path):
        if file.endswith('.doc') or file.endswith('.docx'):
            profile.append(folder)
            file_name.append(file)
        else:
            profile.append(folder)
            file_name.append(file)

resume_data = pd.DataFrame()
resume_data['Profile'] = profile
resume_data['Resumes'] = file_name
resume_data

resume_data.Profile.value_counts().index

resume_data.Profile.value_counts()

"""**Number Of Total Resumes**"""

fig, axe = plt.subplots(1,1, figsize=(15,5), dpi=200)
ax = sns.barplot(x= resume_data.Profile.value_counts().index, y= resume_data.Profile.value_counts(), width=0.5,
                 data= resume_data, ax= axe, label= 'Total Pofile Category : {}'.format(len(resume_data.Profile.unique())))

axe.set_xlabel('Profiles', size=16,fontweight = 'bold')
axe.set_ylabel('Frequency', size=16,fontweight = 'bold')
plt.xticks(rotation = 0)
plt.legend(loc='best', fontsize= 'x-large')
plt.title('Number of Profiles in Resumes', fontsize= 20, fontweight= 'bold')

for i in ax.containers:
    ax.bar_label(i,color = 'black', fontweight = 'bold', fontsize= 12)

pylab.rcParams.update(rcParams)
fig.tight_layout()
plt.show()

fig = plt.figure(figsize=(8,8),dpi=100)

sizes = resume_data.Profile.value_counts()
labels = resume_data.Profile.value_counts().index
colors = ['#FF0000', '#0000FF', '#00FFFF', '#800080', '#FFA500', '#000000', '#FFFF00']
explode = (0.01, 0.01, 0.01, 0.01, 0.01, 0.01)

plt.pie(sizes,
        colors= colors,
        labels= labels, autopct= '%1.0f%%',
        pctdistance=0.85, explode= explode,
        startangle= 0,
        textprops= {'size':'large', 'fontweight':'bold', 'color':'black'})

centre_circle = plt.Circle((0,0), 0.60, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)
plt.title('Percentage of Profiles in Resumes', fontsize= 18, fontweight= 'bold')
plt.legend(labels, loc="center")

pylab.rcParams.update(rcParams)
fig.tight_layout()
plt.show()

"""**Reading a Resume File**"""

def extract_text_from_docx(docx_path):
    txt = docx2txt.process(docx_path)
    if txt:
        return txt.replace('\t', ' ')
    return None

print(extract_text_from_docx('/content/drive/MyDrive/P-257/Internship/Internship_Susovan Bag_Musquare Technologies.docx'))

import docx2txt

def extract_text_from_docx(docx_path):
    txt = docx2txt.process(docx_path)
    if txt:
        return txt.replace('\t', ' ')
    return None

print(extract_text_from_docx('/content/drive/MyDrive/P-257/React JS developer/React Developer_Haripriya.docx'))

"""**Data Exploration**"""

resume_data = pd.read_csv('Cleaned_Resumes.csv')
resume_data

"""**N-Grams-A Sequence of N words**

*  N-grams are continuous sequences of words or symbols, or tokens in a document. In technical terms, they can be defined as the neighboring sequences of items in a document.  
*   An N-gram model is built by counting how often word sequences occur in corpus text and then estimating the probabilities
"""

#UniGrams(1-Grams):Unigrams refer to individual words or tokens in a text.


TextBlob(resume_data['Resume_Details'][1]).ngrams(1)[:20]

#Bi-Grams(2-Grams):Bigrams are sequences of two consecutive words in a text.

TextBlob(resume_data['Resume_Details'][1]).ngrams(2)[:20]

#Tri-Grams(3-grams):Trigrams are sequences of three consecutive words in a text.

TextBlob(resume_data['Resume_Details'][1]).ngrams(3)[:20]

resume_data['Resume_Details']

"""**Top 20 most used words in Resumes**


*   Using Countvectorizer


"""

countvec = CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1,2))
ngrams = countvec.fit_transform(resume_data['Resume_Details']) # matrix of ngrams
count_values = ngrams.toarray().sum(axis=0) # count frequency of ngrams

vocab = countvec.vocabulary_ # list of ngrams
df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k, i in vocab.items()],
                               reverse=True)).rename(columns={0: 'Frequency', 1:'Unigram_Bigram'})

df_ngram.head(20)

fig, axe = plt.subplots(1,1, figsize=(12,6), dpi=200)
ax = sns.barplot(x=df_ngram['Unigram_Bigram'].head(25), y=df_ngram.Frequency.head(25), data=resume_data, ax = axe,
            label='Total Pofile Category : {}'.format(len(resume_data.Category.unique())))

axe.set_xlabel('Words', size=16,fontweight= 'bold')
axe.set_ylabel('Frequency', size=16, fontweight= 'bold')
plt.xticks(rotation = 90)
plt.legend(loc='best', fontsize= 'x-large')
plt.title('Top 25 Most used Words in Resumes', fontsize= 18, fontweight= 'bold')

for i in ax.containers:
    ax.bar_label(i,color = 'black', fontweight = 'bold', fontsize= 12)

pylab.rcParams.update(rcParams)
fig.tight_layout()
plt.show()

"""**Using The WordCLowds**"""

text = " ".join(var for var in resume_data.Resume_Details) # Creating the text variable

word_cloud = WordCloud(width=1000, height=800, random_state=10, background_color="black",
                       colormap="Pastel1", collocations=False, stopwords=STOPWORDS).generate(text)

plt.figure(figsize=(10,8), dpi=500) # Display the generated Word Cloud
plt.title('Most Common Words in Resumes', fontsize= 16, fontweight= 'bold')
plt.imshow(word_cloud)
plt.axis("off")

plt.show()

"""**#########################################################################################################**

##      Model Building
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
sns.set_style('darkgrid')
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab


import warnings
warnings.filterwarnings('ignore')

"""**Import Dataset**"""

resume_data = pd.read_csv('Cleaned_Resumes.csv')
resume_data

resume_data.head()

resume_data.tail()

"""**Data Preprocessing**


*   **LabelEncoder**-LabelEncoder used for encoding categorical labels into numeric labels. It assigns a unique integer value to each unique category or label in the dataset.


"""

from sklearn.preprocessing import LabelEncoder
Encoder=LabelEncoder()
resume_data['LabelEncoding']=Encoder.fit_transform(resume_data['Category'])
resume_data

resume_data.describe()

resume_data.isnull().sum()

"""**Model Building**

*   **Train_Test Split**-It involves splitting the available dataset into two separate subsets: the training set and the test set.
  

  **Train set**-Allowing it to learn patterns and relationships in the data. It is the portion of the dataset on which the model is fitted or trained.


  **Test set**-It is used to evaluate the performance of the trained model. It serves as an independent dataset that the model has not seen during training. By evaluating the model on the test set, we can assess its generalization ability and estimate its performance on unseen data.
"""

from sklearn.model_selection import train_test_split

x  = resume_data['Resume_Details'].values
y  = resume_data['Category'].values

x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=45, test_size=0.25,stratify=y)
x_train.shape, x_test.shape

"""**TF-IDF Vectorization-(Term Frequency-Inverse Document Frequency) vectorization**

*   **Term Frequency[TF]**-It calculates the frequency of each term (word) within a document. It gives an indication of how often a term appears in the document.
*   **Inverse Document Frequency[IDF]**- It measures the importance of a term across a corpus of documents. It is calculated as the logarithm of the ratio between the total number of documents and the number of documents that contain the term. Terms that are common across all documents receive a lower IDF score, while terms that are unique to a few documents receive a higher IDF score.


*   **TF-IDF**: It combines the term frequency and inverse document frequency to calculate a weight for each term in a document. It is calculated as the product of TF and IDF. TF-IDF assigns higher weights to terms that are frequent within a document but rare across the entire corpus, indicating their importance in distinguishing the document.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vector = TfidfVectorizer(sublinear_tf=True,stop_words='english')

x_train = tfidf_vector.fit_transform(x_train)
x_test = tfidf_vector.transform(x_test)

x_train.shape, x_test.shape

print(y_train),print(y_test)

"""**Classification Models**

**Logistic Regression**

**Logistic Regression**-It is used for binary classification by estimating the probability of an event occurring based on input variables.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import f1_score, classification_report, precision_score, recall_score

lr = LogisticRegression(class_weight ='balanced',multi_class='ovr',solver='lbfgs',C=100.0)
lr.fit(x_train, y_train)
lr_train_predict=lr.predict(x_train)
lr_prediction = lr.predict(x_test)
lr_score = lr.score(x_test,y_test)
print("Logistic Regression Train Accuracy: {}%".format(round(lr.score(x_train,y_train)*100,2)))
print("Logistic Regression Test Accuracy: {}%".format(round(lr.score(x_test,y_test)*100,2)))
lr_cm = confusion_matrix(y_test, lr_prediction)
print("Classification Report:Train Data\n")
print(classification_report(y_train, lr_train_predict))
print("-----------------------------------------------------------\n")
print("Classification Report:Test Data\n")
print(classification_report(y_test, lr_prediction))

clf_report1 = classification_report(y_test, lr_prediction,
                                   labels=None,
                                   target_names=None,
                                   output_dict=True)

plt.figure(figsize=(8,4))
sns.heatmap(pd.DataFrame(clf_report1).T, annot=True,cmap="twilight_shifted")
plt.title("Logistic Regression Classification Report")
plt.show()

"""**KNN CLASSIFIER**

**KNN CLASSIFIER**-Predicts the class label of a new data point based on the class labels of its nearest neighbors


*  **K**: K refers to the number of nearest neighbors that are considered for classification.
*  **Nearest Neighbour**:Nearest neighbors are the data points in the training set that are closest to the new data point in terms of distance.
*The Distance is calculated in **Ecluidean distance**
* **Classifier**:Assigns class labels to data instances based on their features
"""

from sklearn.neighbors import KNeighborsClassifier

knn =KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train, y_train)
knn_train_predict=knn.predict(x_train)

knn_prediction = knn.predict(x_test)
knn_score = knn.score(x_test, y_test)
print("KNN Classification Train Accuracy: {}%".format(round(knn.score(x_train,y_train)*100,2)))
print("KNN Classification Test Accuracy: {}%".format(round(knn.score(x_test,y_test)*100,2)))
knn_cm = confusion_matrix(y_test, knn_prediction)
print('Classification Report:Train data\n')
print(classification_report(y_train, knn_train_predict))
print("------------------------------------------------------------\n")
print('Classification Report:Test data\n')

print(classification_report(y_test, knn_prediction))

clf_report2 = classification_report(y_test, knn_prediction,
                                   labels=None,
                                   target_names=None,
                                   output_dict=True)

plt.figure(figsize=(8,4))
sns.heatmap(pd.DataFrame(clf_report2).T, annot=True,cmap="twilight_shifted")
plt.title("KNN CLASSIFIER")
plt.show()

"""**DECISION TREE CLASSIFIER**"""

from sklearn.tree import DecisionTreeClassifier

from sklearn.multiclass import OneVsRestClassifier

dt = OneVsRestClassifier(DecisionTreeClassifier(criterion='entropy',
                                                class_weight = "balanced",splitter='best',max_depth=None))
dt.fit(x_train, y_train)
dt_train_predict=dt.predict(x_train)
dt_prediction = dt.predict(x_test)
dt_score = dt.score(x_test, y_test)
print("Decision Tree Classification Train Accuracy: {}%".format(round(dt.score(x_train,y_train)*100,2)))
print("Decision Tree Classification Test Accuracy: {}%".format(round(dt.score(x_test,y_test)*100,2)))
dt_cm = confusion_matrix(y_test, dt_prediction)
print("Classification Report:Train data\n")
print(classification_report(y_train,dt_train_predict))
print("----------------------------------------------------\n")
print("Classification Report:Test data\n")
print(classification_report(y_test, dt_prediction))

clf_report3 = classification_report(y_test, dt_prediction,
                                   labels=None,
                                   target_names=None,
                                   output_dict=True)

plt.figure(figsize=(8,4))
sns.heatmap(pd.DataFrame(clf_report3).T, annot=True,cmap="twilight_shifted")
plt.title("")
plt.show()

"""**RANDOM FOREST CLASSIFIER**"""

from sklearn.ensemble import RandomForestClassifier

rf =RandomForestClassifier(n_estimators=200,criterion='entropy',max_depth=5,max_features='auto',random_state=None,
 class_weight="balanced")
rf.fit(x_train, y_train)
rf_prediction = rf.predict(x_test)
rf_score = rf.score(x_test, y_test)
print("Random Forest Classification Train Accuracy: {}%".format(round(rf.score(x_train,y_train)*100,2)))
print("Random Forest Classification Test Accuracy: {}%".format(round(rf.score(x_test,y_test)*100,2)))
rf_cm = confusion_matrix(y_test, rf_prediction)
print("Classification Report:\n")
print(classification_report(y_test, rf_prediction))

clf_report4 = classification_report(y_test, rf_prediction,
                                   labels=None,
                                   target_names=None,
                                   output_dict=True)

plt.figure(figsize=(8,4))
sns.heatmap(pd.DataFrame(clf_report4).T, annot=True,cmap="nipy_spectral")
plt.title("Random Forest Classification Report")
plt.show()

"""**ADABOOST CLASSIFIER**"""

from sklearn.ensemble import AdaBoostClassifier

model_Adaboost = AdaBoostClassifier(n_estimators=100)
model_Adaboost.fit(x_train, y_train)
y_pred = model_Adaboost.predict(x_test)
accuracy_Adaboost = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_Adaboost.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_Adaboost.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_Adaboost,classification_report(y_test, y_pred)))
nb_score = model_Adaboost.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_Adaboost = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_Adaboost = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_Adaboost = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_Adaboost = round(accuracy_score(y_test,y_pred),2)

clf_report5 = classification_report(y_test, y_pred,
                                   labels=None,
                                   target_names=None,
                                   output_dict=True)

plt.figure(figsize=(10,4))
sns.heatmap(pd.DataFrame(clf_report5).T, annot=True,cmap="cividis")
plt.title("XGB Classification Report")
plt.show()

"""**SUPPORT VECTOR MACHINE(SVM) CLASSIFIER**"""

from sklearn.svm import SVC

model_svm = SVC()
model_svm.fit(x_train, y_train)
y_pred = model_svm.predict(x_test)
accuracy_svm = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_svm.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_svm.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_svm,classification_report(y_test, y_pred)))
nb_score = model_svm.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_svm = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_svm = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_svm = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_svm = round(accuracy_score(y_test,y_pred),2)

svm =OneVsRestClassifier(SVC(C=1.0,kernel='linear',degree=3,gamma='scale',class_weight ='balanced'))
svm.fit(x_train, y_train)
svm_train_predict=svm.predict(x_train)
svm_prediction = svm.predict(x_test)
svm_score = svm.score(x_test, y_test)
print("SVM Classification Train Accuracy: {}%".format(round(svm.score(x_train,y_train)*100,2)))
print("SVM Classification Test Accuracy: {}%".format(round(svm.score(x_test,y_test)*100,2)))
svm_cm = confusion_matrix(y_test, svm_prediction)
print("Classification Report:Train data\n")
print(classification_report(y_train, svm_train_predict))
print("---------------------------------------------------------\n")
print("Classification Report:Test data\n")
print(classification_report(y_test, svm_prediction))

clf_report6 = classification_report(y_test, svm_prediction,
                                   labels=None,
                                   target_names=None,
                                   output_dict=True)

plt.figure(figsize=(8,4))
sns.heatmap(pd.DataFrame(clf_report6).T, annot=True,cmap="rocket")
plt.title("SVM Classification Report")
plt.show()

"""**BAGGING CLASSIFIER**"""

from sklearn.ensemble import BaggingClassifier

model_bagg = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)
model_bagg.fit(x_train, y_train)
y_pred = model_bagg.predict(x_test)
accuracy_bagg = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_bagg.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_bagg.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_bagg,classification_report(y_test, y_pred)))
nb_score = model_bagg.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_bagg = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_bagg = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_bagg = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_bagg = round(accuracy_score(y_test,y_pred),2)

clf_report7 = classification_report(y_test, y_pred,
                                   labels=None,
                                   target_names=None,
                                   output_dict=True)

plt.figure(figsize=(8,4))
sns.heatmap(pd.DataFrame(clf_report7).T, annot=True,cmap="cividis")
plt.title("Bagging Classification Report")
plt.show()

"""**GRADIENT BOOSTING CLASSIFIER**"""

from sklearn.ensemble import GradientBoostingClassifier

model_GradientBoost = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1, random_state=0)
model_GradientBoost.fit(x_train, y_train)
y_pred = model_GradientBoost.predict(x_test)
accuracy_GradientBoost = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_GradientBoost.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_GradientBoost.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_GradientBoost,classification_report(y_test, y_pred)))
nb_score = model_GradientBoost.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_Gradientboost = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_Gradientboost = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_Gradientboost = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_Gradientboost = round(accuracy_score(y_test,y_pred),2)

clf_report8 = classification_report(y_test, y_pred,
                                   labels=None,
                                   target_names=None,
                                   output_dict=True)

plt.figure(figsize=(8,4))
sns.heatmap(pd.DataFrame(clf_report8).T, annot=True,cmap="cividis")
plt.title("GradientBoosting Classification Report")
plt.show()

"""**NAIVE BAYES CLASSIFIER**"""

from sklearn.naive_bayes import MultinomialNB

model_NB =MultinomialNB(alpha=1, fit_prior=False, class_prior=None)
model_NB.fit(x_train, y_train)
y_pred = model_NB.predict(x_test)
accuracy_NB = accuracy_score(y_test, y_pred)
print('Accuracy of training set : {:.2f}'.format(model_NB.score(x_train, y_train)))
print('Accuracy of  test set    : {:.2f}'.format(model_NB.score(x_test, y_test)))
print("Classification report for classifier %s:\n%s\n" % (model_NB,classification_report(y_test, y_pred)))
nb_score = model_NB.score(x_test, y_test)
nb_cm = confusion_matrix(y_test, y_pred)

precision_NB = round(precision_score(y_test,y_pred,average = 'macro'),2)
recall_NB = round(recall_score(y_test,y_pred, average = 'macro'),2)
f1_NB = round(f1_score(y_test,y_pred, average = 'macro'),2)
accuracy_NB = round(accuracy_score(y_test,y_pred),2)

clf_report9 = classification_report(y_test, y_pred,
                                   labels=None,
                                   target_names=None,
                                   output_dict=True)

plt.figure(figsize=(8,4))
sns.heatmap(pd.DataFrame(clf_report9).T, annot=True,cmap="cividis")
plt.title("NavieBayes Classification Report")
plt.show()





















